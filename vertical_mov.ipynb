{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load video\n",
    "video_path = \"combscroll.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get frame rate and total frames\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))  # Original FPS\n",
    "frame_interval = fps  # Process one frame per second\n",
    "frame_count = 0\n",
    "scroll_timestamps = []\n",
    "scrolling = False  # Flag to track scrolling start and end\n",
    "\n",
    "# Read the first frame\n",
    "ret, prev_frame = cap.read()\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY) if ret else None\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Set video position to extract frame at 1-second intervals\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_count)\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Compute Optical Flow (Farneback method)\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    prev_gray = gray  # Update previous frame\n",
    "\n",
    "    # Extract vertical movement component\n",
    "    vertical_movement = flow[..., 1]  # Y-axis movement\n",
    "\n",
    "    # Compute mean absolute vertical movement\n",
    "    avg_vertical_flow = np.mean(np.abs(vertical_movement))\n",
    "\n",
    "    # Define scrolling threshold (tune this based on your video)\n",
    "    SCROLL_THRESHOLD = 1.5  # Adjust for sensitivity\n",
    "\n",
    "    # Get the current timestamp in seconds\n",
    "    current_time = frame_count / fps\n",
    "\n",
    "    if avg_vertical_flow > SCROLL_THRESHOLD:\n",
    "        if not scrolling:\n",
    "            scroll_timestamps.append((current_time, None))\n",
    "            scrolling = True\n",
    "    else:\n",
    "        if scrolling:\n",
    "            scroll_timestamps[-1] = (scroll_timestamps[-1][0], current_time)\n",
    "            scrolling = False\n",
    "\n",
    "    # Move to the next second (skip frames for efficiency)\n",
    "    frame_count += frame_interval\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print scrolling timestamps\n",
    "for start, end in scroll_timestamps:\n",
    "    print(f\"Scroll detected from {start:.2f}s to {end:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe try to use only the center part of the video?\n",
    "# ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No frames grabbed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "video_path = \"combscroll.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "# params for ShiTomasi corner detection\n",
    "feature_params = dict( maxCorners = 100,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (15, 15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "# Create some random colors\n",
    "color = np.random.randint(0, 255, (100, 3))\n",
    "# Take first frame and find corners in it\n",
    "ret, old_frame = cap.read()\n",
    "old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "p0 = cv2.goodFeaturesToTrack(old_gray, mask = None, **feature_params)\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(old_frame)\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('No frames grabbed!')\n",
    "        break\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # calculate optical flow\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "    # Select good points\n",
    "    if p1 is not None:\n",
    "        good_new = p1[st==1]\n",
    "        good_old = p0[st==1]\n",
    "    # draw the tracks\n",
    "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = new.ravel()\n",
    "        c, d = old.ravel()\n",
    "        mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), color[i].tolist(), 2)\n",
    "        frame = cv2.circle(frame, (int(a), int(b)), 5, color[i].tolist(), -1)\n",
    "    img = cv2.add(frame, mask)\n",
    "    cv2.imshow('frame', img)\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "    # Now update the previous frame and previous points\n",
    "    old_gray = frame_gray.copy()\n",
    "    p0 = good_new.reshape(-1, 1, 2)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'modified_videos'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load video\n",
    "video_path = \"combscroll.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "output_path = \"modified_videos\"\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Define center crop region (e.g., middle 50% of width and height)\n",
    "crop_x_start = frame_width // 4\n",
    "crop_x_end = 3 * frame_width // 4\n",
    "crop_y_start = frame_height // 4\n",
    "crop_y_end = 3 * frame_height // 4\n",
    "\n",
    "# Params for ShiTomasi corner detection\n",
    "feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "\n",
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2,\n",
    "                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Create some random colors\n",
    "color = np.random.randint(0, 255, (100, 3))\n",
    "\n",
    "# Take first frame and find corners in the center region\n",
    "ret, old_frame = cap.read()\n",
    "if not ret:\n",
    "    raise ValueError(\"Failed to read video.\")\n",
    "\n",
    "# Convert to grayscale\n",
    "old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Crop to center\n",
    "old_gray_center = old_gray[crop_y_start:crop_y_end, crop_x_start:crop_x_end]\n",
    "\n",
    "# Detect features in the cropped region\n",
    "p0 = cv2.goodFeaturesToTrack(old_gray_center, mask=None, **feature_params)\n",
    "\n",
    "# Adjust coordinates to match original frame\n",
    "if p0 is not None:\n",
    "    p0[:, 0, 0] += crop_x_start\n",
    "    p0[:, 0, 1] += crop_y_start\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(old_frame)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert to grayscale\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame_gray_center = frame_gray[crop_y_start:crop_y_end, crop_x_start:crop_x_end]\n",
    "\n",
    "    # Calculate optical flow in the center region\n",
    "    if p0 is not None:\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "\n",
    "        if p1 is not None:\n",
    "            good_new = p1[st == 1]\n",
    "            good_old = p0[st == 1]\n",
    "\n",
    "            # Draw the tracks only in the center region\n",
    "            for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "                a, b = new.ravel()\n",
    "                c, d = old.ravel()\n",
    "                if crop_x_start <= a <= crop_x_end and crop_y_start <= b <= crop_y_end:\n",
    "                    mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), color[i].tolist(), 2)\n",
    "                    frame = cv2.circle(frame, (int(a), int(b)), 5, color[i].tolist(), -1)\n",
    "\n",
    "            p0 = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "    # Overlay the mask on the frame\n",
    "    img = cv2.add(frame, mask)\n",
    "\n",
    "    # Write the frame to output video\n",
    "    out.write(img)\n",
    "\n",
    "    # Update previous frame\n",
    "    old_gray = frame_gray.copy()\n",
    "\n",
    "# Release everything\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Provide the download link\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Set file paths\n",
    "input_video_path = \"combscroll.mp4\"\n",
    "output_video_path = \"C:/Users/supia/Documents/GitHub/Accessibility-AI-and-Video/modified_videos/cropped_video.mp4\"\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "\n",
    "# Define cropping region (center 50% of width and height)\n",
    "crop_x_start = frame_width // 4\n",
    "crop_x_end = 3 * frame_width // 4\n",
    "crop_y_start = frame_height // 4\n",
    "crop_y_end = 3 * frame_height // 4\n",
    "cropped_width = crop_x_end - crop_x_start\n",
    "cropped_height = crop_y_end - crop_y_start\n",
    "\n",
    "# Create a VideoWriter object\n",
    "out = cv2.VideoWriter(output_video_path, fps, fourcc, (640, 480))\n",
    "\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break  # End of video\n",
    "#     out.write(frame)\n",
    "\n",
    "# cap.release()\n",
    "# out.release()\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't receive frame (stream end?). Exiting ...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    " \n",
    "cap = cv.VideoCapture(\"combscroll.mp4\")\n",
    "output_video_path = \"C:/Users/supia/Documents/GitHub/Accessibility-AI-and-Video/modified_videos/testing_modified.mp4\"\n",
    " \n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv.VideoWriter_fourcc(*'XVID')\n",
    "out = cv.VideoWriter(output_video_path, fourcc, 20.0, (640,  480))\n",
    " \n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "    frame = cv.flip(frame, 0)\n",
    " \n",
    "    # write the flipped frame\n",
    "    out.write(frame)\n",
    " \n",
    "    cv.imshow('frame', frame)\n",
    "    if cv.waitKey(1) == ord('q'):\n",
    "        break\n",
    " \n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't receive frame (stream end?). Exiting ...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    " \n",
    "cap = cv.VideoCapture('combscroll.mp4')\n",
    " \n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    " \n",
    "    # if frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    " \n",
    "    cv.imshow('frame', gray)\n",
    "    if cv.waitKey(1) == ord('q'):\n",
    "        break\n",
    " \n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m bgr \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mcvtColor(hsv, cv\u001b[38;5;241m.\u001b[39mCOLOR_HSV2BGR)\n\u001b[0;32m     19\u001b[0m cv\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe2\u001b[39m\u001b[38;5;124m'\u001b[39m, bgr)\n\u001b[1;32m---> 20\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xff\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m27\u001b[39m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "cap = cv.VideoCapture(\"combscroll.mp4\")\n",
    "ret, frame1 = cap.read()\n",
    "prvs = cv.cvtColor(frame1, cv.COLOR_BGR2GRAY)\n",
    "hsv = np.zeros_like(frame1)\n",
    "hsv[..., 1] = 255\n",
    "while(1):\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        print('No frames grabbed!')\n",
    "        break\n",
    "    next = cv.cvtColor(frame2, cv.COLOR_BGR2GRAY)\n",
    "    flow = cv.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    mag, ang = cv.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    hsv[..., 0] = ang*180/np.pi/2\n",
    "    hsv[..., 2] = cv.normalize(mag, None, 0, 255, cv.NORM_MINMAX)\n",
    "    bgr = cv.cvtColor(hsv, cv.COLOR_HSV2BGR)\n",
    "    cv.imshow('frame2', bgr)\n",
    "    k = cv.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "    elif k == ord('s'):\n",
    "        cv.imwrite('opticalfb.png', frame2)\n",
    "        cv.imwrite('opticalhsv.png', bgr)\n",
    "    prvs = next\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mnext\u001b[39m \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame2, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Calculate optical flow between the two frames\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m flow \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalcOpticalFlowFarneback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Normalize horizontal and vertical components\u001b[39;00m\n\u001b[0;32m     33\u001b[0m horz \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mnormalize(flow[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mNORM_MINMAX)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Replace with your video file path\n",
    "video_path = 'croppedvideo.mp4'\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Read the first frame\n",
    "ret, frame1 = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read video.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "frame_counter = 1\n",
    "\n",
    "while True:\n",
    "    # Read the next frame\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate optical flow between the two frames\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    # Normalize horizontal and vertical components\n",
    "    # horz = cv2.normalize(flow[...,0], None, 0, 255, cv2.NORM_MINMAX)\n",
    "    vert = cv2.normalize(flow[...,1], None, 0, 255, cv2.NORM_MINMAX)\n",
    "    # horz = horz.astype('uint8')\n",
    "    vert = vert.astype('uint8')\n",
    "\n",
    "    # Show the components as images\n",
    "    # cv2.imshow('Horizontal Component', horz)\n",
    "    cv2.imshow('Vertical Component', vert)\n",
    "\n",
    "    # Set the current frame as previous for the next iteration\n",
    "    prvs = next.copy()\n",
    "\n",
    "    # Wait for a key press\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "    elif k == ord('s'):\n",
    "        # cv2.imwrite(f'opticalflow_horz{frame_counter}.pgm', horz)\n",
    "        cv2.imwrite(f'opticalflow_vert{frame_counter}.pgm', vert)\n",
    "\n",
    "    frame_counter += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to read video.\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m     15\u001b[0m     exit()\n\u001b[1;32m---> 17\u001b[0m prvs \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m frame_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Step size for drawing fewer flow lines for better readability\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Replace with your video file path\n",
    "video_path = 'your_video.mp4'\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Read the first frame\n",
    "ret, frame1 = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read video.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "frame_counter = 1\n",
    "\n",
    "# Step size for drawing fewer flow lines for better readability\n",
    "step = 16\n",
    "\n",
    "while True:\n",
    "    # Read the next frame\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate optical flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    # Create a copy of the current frame for visualization\n",
    "    vis = frame2.copy()\n",
    "\n",
    "    # Draw flow vectors\n",
    "    h, w = next.shape\n",
    "    for y in range(0, h, step):\n",
    "        for x in range(0, w, step):\n",
    "            fx, fy = flow[y, x]\n",
    "            end_point = (int(x + fx), int(y + fy))\n",
    "            cv2.arrowedLine(vis, (x, y), end_point, color=(0, 255, 0), thickness=1, tipLength=0.4)\n",
    "\n",
    "    # Display result\n",
    "    cv2.imshow('Optical Flow Visualization', vis)\n",
    "\n",
    "    # Prepare for next iteration\n",
    "    prvs = next.copy()\n",
    "\n",
    "    # Handle key input\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:  # Esc to exit\n",
    "        break\n",
    "    elif k == ord('s'):\n",
    "        cv2.imwrite(f'opticalflow_frame{frame_counter}.png', vis)\n",
    "\n",
    "    frame_counter += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
